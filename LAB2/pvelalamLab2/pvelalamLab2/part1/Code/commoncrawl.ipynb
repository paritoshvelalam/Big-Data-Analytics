{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paritosh Kumar Velalam - pvelalam\n",
    "Gaurav Avula - gauravav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "try:\n",
    "    from cStringIO import StringIO\n",
    "except:\n",
    "    from StringIO import StringIO\n",
    "keywords1 = (\"Baseball\", \"baseball\", \"BASEBALL\")\n",
    "keywords2 = (\"NFL\",\"N.F.L.\")\n",
    "keywords3 = (\"Basketball\",\"NBA\",\"N.B.A.\", \"BASKETBALL\")\n",
    "keywords4 = (\"Soccer\", \"soccer\" \"SOCCER\")\n",
    "keywords5 = (\"Tennis\", \"tennis\")\n",
    "final_list = []\n",
    "# Coomon crawl server ALPI parameters\n",
    "cc_filter = 'filter==status:200'\n",
    "cc_matchtype = 'matchType=domain' \n",
    "cc_url= 'www.espn.com'\n",
    "index = (\"04\",\"09\",\"13\")\n",
    "for j in index:\n",
    "    resp = requests.get('https://index.commoncrawl.org/CC-MAIN-2019-'+j+'-index?'+'url='+cc_url+'&'+cc_matchtype+'&'+cc_filter+'&'+'output=json')\n",
    "    pages = [json.loads(x) for x in resp.content.strip().split('\\n')]\n",
    "    print (pages[0])\n",
    "    print len(pages)\n",
    "    for i in range(0,len(pages)):\n",
    "        page = pages[i]\n",
    "        offset, length = int(page['offset']), int(page['length'])\n",
    "        offset_end = offset + length - 1\n",
    "        prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "        resp = requests.get(prefix + page['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "        raw_data = StringIO(resp.content)\n",
    "        f = gzip.GzipFile(fileobj=raw_data)\n",
    "        data = f.read()\n",
    "        try:\n",
    "            warc, header, response = data.strip().split('\\r\\n\\r\\n',2)    \n",
    "            soup = BeautifulSoup(response)\n",
    "            text_iter = soup.findAll('p')\n",
    "            finaltext = \"\"\n",
    "            for tag in text_iter:\n",
    "                finaltext = finaltext + tag.getText().encode(\"UTF-8\")\n",
    "            if any(s in finaltext for s in keywords1):\n",
    "                with open(\"cc_espn_Baseball.txt\",\"a\") as text_file:\n",
    "                    text_file.write(finaltext+\"\\n\")\n",
    "                with open(\"cc_espn_links_Baseball.csv\",\"a\") as f:\n",
    "                    pagearr = []\n",
    "                    pagearr.append(pages[i])\n",
    "                    df = pd.DataFrame(pagearr)\n",
    "                    df.to_csv(f, index=False, header=f.tell()==0)\n",
    "            if any(s in finaltext for s in keywords2):\n",
    "                with open(\"cc_espn_NFL.txt\",\"a\") as text_file:\n",
    "                    text_file.write(finaltext+\"\\n\")\n",
    "                with open(\"cc_espn_links_NFL.csv\",\"a\") as f:\n",
    "                    pagearr = []\n",
    "                    pagearr.append(pages[i])\n",
    "                    df = pd.DataFrame(pagearr)\n",
    "                    df.to_csv(f, index=False, header=f.tell()==0)\n",
    "            if any(s in finaltext for s in keywords3):\n",
    "                with open(\"cc_espn_Basketball.txt\",\"a\") as text_file:\n",
    "                    text_file.write(finaltext+\"\\n\")\n",
    "                with open(\"cc_espn_links_Basketball.csv\",\"a\") as f:\n",
    "                    pagearr = []\n",
    "                    pagearr.append(pages[i])\n",
    "                    df = pd.DataFrame(pagearr)\n",
    "                    df.to_csv(f, index=False, header=f.tell()==0)\n",
    "            if any(s in finaltext for s in keywords4):\n",
    "                with open(\"cc_espn_Soccer.txt\",\"a\") as text_file:\n",
    "                    text_file.write(finaltext+\"\\n\")\n",
    "                with open(\"cc_espn_links_Soccer.csv\",\"a\") as f:\n",
    "                    pagearr = []\n",
    "                    pagearr.append(pages[i])\n",
    "                    df = pd.DataFrame(pagearr)\n",
    "                    df.to_csv(f, index=False, header=f.tell()==0)\n",
    "            if any(s in finaltext for s in keywords5):\n",
    "                with open(\"cc_espn_Tennis.txt\",\"a\") as text_file:\n",
    "                    text_file.write(finaltext+\"\\n\")\n",
    "                with open(\"cc_espn_links_Tennis.csv\",\"a\") as f:\n",
    "                    pagearr = []\n",
    "                    pagearr.append(pages[i])\n",
    "                    df = pd.DataFrame(pagearr)\n",
    "                    df.to_csv(f, index=False, header=f.tell()==0)\n",
    "        except BaseException:\n",
    "            print i\n",
    "            print \"exception caught\"\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
